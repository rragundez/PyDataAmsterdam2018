{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"//fonts.googleapis.com/css?family=Quicksand:300\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"//fonts.googleapis.com/css?family=Quicksand:300\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from keras import datasets\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from utils import plot_training_summary\n",
    "from utils import TimeSummary\n",
    "from utils import set_seed\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 15, 6\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build your first convolutional neural network\n",
    "\n",
    "- Rodrigo Agundez\n",
    "- Amsterdam @ Booking.com\n",
    "- Friday May 25th, 2018\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Requirements for this notebook:\n",
    " - matplotlib\n",
    " - keras\n",
    " - tensorflow\n",
    " \n",
    "Goal\n",
    "\n",
    "- Build a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "This Notebook contains some questions labeled __Homework__.\n",
    "Ignore them now but do try to answer them later if you want to get more in-depth understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many other libraries, `keras` includes some standard datasets to play around with.\n",
    "The follow a specific [API](https://www.tensorflow.org/programmers_guide/datasets) that make them iteract nicely with TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've tried this Notebook and made sense of the model, load the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Train:\\tX shape:{X_train.shape}\\tY shape:{y_train.shape}\\tType (X, y): ({X_train.dtype}, {y_train.dtype})\\tX values (max, min): ({X_train.min()}, {X_train.max()})\")\n",
    "print(f\"Test:\\tX shape:{X_test.shape}\\tY shape:{y_test.shape}\\tType (X, y): ({X_test.dtype}, {y_test.dtype})\\tX values (max, min): ({X_test.min()}, {X_test.max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Dataset summary\n",
    ">\n",
    "- How many training examples do we have?\n",
    "- How many color channels does each picture have?\n",
    "- What will the input size to the DNN be?\n",
    "- What will the output size of the DNN be? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ax.imshow(random.choice(X_train), cmap='gray') for ax in plt.subplots(1, 6)[1]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Since we are using a convolutional network we do not need to flatten the array into 1D. We actually have to add a dimension for the channels.\n",
    "- rescale pixel values between 0 and 1\n",
    "- Input type should be float\n",
    "- There are 10 classes so in order to compute the cross entropy loss function we need to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_train.shape) != 4:\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "if len(X_test.shape) != 4:\n",
    "    X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "X_train, X_test = X_train.astype('float') / 255, X_test.astype('float') / 255\n",
    "y_train_onehot, y_test_onehot = to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the resulting dimensions and types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train:\\tX shape:{X_train.shape}\\tY shape:{y_train_onehot.shape}\\tType (X, y): ({X_train.dtype}, {y_train_onehot.dtype})\\tX values (min, max): ({X_train.min()}, {X_train.max()})\")\n",
    "print(f\"Test:\\tX shape:{X_test.shape}\\tY shape:{y_test_onehot.shape}\\tType (X, y): ({X_test.dtype}, {y_test_onehot.dtype})\\tX values (min, max): ({X_test.min()}, {X_test.max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Model input & output\n",
    "- Are the input and output sizes correct from what you thought?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (solution in `utils.py`)\n",
    "\n",
    "Construct a model with the instructions:\n",
    "\n",
    "- inputs are normalized using `BatchNormaliation` followed by a `Dropout` layer with a rate of 0.3\n",
    "- then add a [2D convolutional layer](https://keras.io/layers/convolutional/) with a kernel of 3x3\n",
    "- output from the covolutional layer goes through a [`MaxPooling` layer](https://keras.io/layers/pooling/)\n",
    "- then `Flatten` the output and add a `Dropout` layer with a rate of 0.3\n",
    "- connect the output to a `Dense` layer\n",
    "- followed by a `BatchNormalization` and `relu` activation function\n",
    "- then a `DropoutLayer`\n",
    "- and finally connect to the output layer with an `softmax` activation function\n",
    "\n",
    "doen forget to:\n",
    "- use the `relu` activation function or others\n",
    "- use `Dropout` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_CNN_model():\n",
    "    model = Sequential()\n",
    "    # input layer transformation (BatchNormalization + Dropout)\n",
    "    \n",
    "    # convolutional layer (Conv2D + MaxPooling2D + Flatten + Dropout)\n",
    "    \n",
    "    # fully connected layer (Dense + BatchNormalization + Activation + Dropout)\n",
    "    \n",
    "    # output layer (Dense + BatchNormalization + Activation)\n",
    "\n",
    "    return model\n",
    "\n",
    "make_CNN_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** can you explain where are the number of parameters for each layer coming from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_summary = TimeSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123) # for reproducibility\n",
    "\n",
    "model = make_CNN_model()\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "summary = model.fit(\n",
    "    X_train, y_train_onehot,\n",
    "    batch_size=10000,\n",
    "    epochs=3,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    callbacks=[time_summary]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test_onehot, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "plot_training_summary(summary, time_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
